{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d683356d",
   "metadata": {},
   "source": [
    "# Web Scraping Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc9c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bb0816",
   "metadata": {},
   "source": [
    "# Web Scraping yallakora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac7bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "5/5/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a52c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = input(\"Please enter a Date in the following format MM/DD/YYYY: \")\n",
    "page = requests.get(f\"https://www.yallakora.com/match-center/%D9%85%D8%B1%D9%83%D8%B2-%D8%A7%D9%84%D9%85%D8%A8%D8%A7%D8%B1%D9%8A%D8%A7%D8%AA?date={date}\")\n",
    "\n",
    "def main(page):\n",
    "\n",
    "    src = page.content\n",
    "    soup = BeautifulSoup(src, \"lxml\")\n",
    "    matches_details=[]\n",
    "\n",
    "    championships = soup.find_all(\"div\", {\"class\": \"matchCard\"})\n",
    "    \n",
    "    \n",
    "    def get_match_info(championships):\n",
    "        championship_title = championships.contents[1].find(\"h2\").text.strip()\n",
    "        all_matches = championships.contents[3].find_all(\"div\", {\"class\": \"liItem\"})\n",
    "        number_of_matches = len(all_matches)\n",
    "\n",
    "        for i in range(number_of_matches):\n",
    "            # get teams names\n",
    "            team_A = all_matches[i].find('div', {'class' : 'teamA'}).text.strip()\n",
    "            team_B = all_matches[i].find('div', {'class' : 'teamB'}).text.strip()\n",
    "\n",
    "            # get score\n",
    "            match_result = all_matches[i].find('div', {'class': 'MResult'}).find_all('span',{'class':'score'})\n",
    "            score = f\"{match_result[0].text.strip()}-{match_result[1].text.strip()}\"\n",
    "\n",
    "            # get match time\n",
    "            match_time = all_matches[i].find('div', {'class': 'MResult'}).find('span', {'class':'time'}).text.strip()\n",
    "        \n",
    "            # add match info to matches_details\n",
    "            matches_details.append({\"نوع البطولة\":championship_title,\"الفريق الاول\":team_A,\"الفريق الثاني\":team_B, \"ميعاد المباراة\":match_time,\"النتيجة\":score})\n",
    "    \n",
    "    \n",
    "    for i in range(len(championships)):\n",
    "        get_match_info(championships[i])\n",
    "    \n",
    "    \n",
    "    keys = matches_details[0].keys()\n",
    "\n",
    "    with open('./Web.csv', 'w',encoding=\"UTF-8\") as output_file:\n",
    "        dict_writer =csv.DictWriter(output_file, keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(matches_details)\n",
    "        print(\"file created\")\n",
    "\n",
    "\n",
    "main(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034816e2",
   "metadata": {},
   "source": [
    "# Alhajery Pharmacy Web Scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee8e76",
   "metadata": {},
   "source": [
    "***Based on Brand Name***\n",
    "\n",
    "Example:\n",
    "* beesline = Beesline\n",
    "* fisher-price = Avene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b76352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_Name = input(\"Please enter a Brand Name: \")\n",
    "page = requests.get(f\"https://alhajerypharmacy.com/zone_e_en/brand/{Brand_Name}?product_list_limit=36\")\n",
    "\n",
    "def main(page):\n",
    "\n",
    "    src = page.content\n",
    "    soup = BeautifulSoup(src, \"lxml\")\n",
    "    product_details=[]\n",
    "\n",
    "    all_products = soup.find_all(\"div\",{'class' : 'product-item-info'})\n",
    "    number_of_product = len(all_products)\n",
    "    print (number_of_product)\n",
    "    \n",
    "    for i in range(number_of_product-1):\n",
    "        # get product names\n",
    "        product_name = all_products[i].find('div', {'class' : 'product-item-details'}).find('strong', {'class':'product-item-name'}).text.strip()\n",
    "        #print (product_name)\n",
    "        \n",
    "        # get product price\n",
    "        price = all_products[i].find('span', {'class' : 'price'}).text.strip()\n",
    "        #print (price)\n",
    "        \n",
    "        # get photo link\n",
    "        \n",
    "        # add match info to matches_details\n",
    "        product_details.append({'Product Name':product_name,\"Price\":price})\n",
    "    \n",
    "    \n",
    "    keys = product_details[0].keys()\n",
    "\n",
    "    with open('./Web7.csv', 'w',encoding=\"UTF-8\") as output_file:\n",
    "        dict_writer =csv.DictWriter(output_file, keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(product_details)\n",
    "        print(\"file created\")\n",
    "    \n",
    "    \n",
    "main(page)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b495ae0",
   "metadata": {},
   "source": [
    "# Books to Scrape - Web Scraping (Multiple Pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330555d6",
   "metadata": {},
   "source": [
    "https://books.toscrape.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab7eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = []\n",
    "\n",
    "for i in range(1,50):\n",
    "    url = f\"https://books.toscrape.com/catalogue/category/books_1/page-{i}.html\"\n",
    "    page = requests.get(url)\n",
    "    src = page.content\n",
    "    soup = BeautifulSoup(src, 'html.parser')\n",
    "    \n",
    "    \n",
    "    ol = soup.find('ol')\n",
    "    articles = ol.find_all('article', class_='product_pod')\n",
    "    \n",
    "    \n",
    "    for article in articles:\n",
    "        image = article.find('img')\n",
    "        title = image.attrs['alt']\n",
    "        starTag = article.find('p')\n",
    "        star = starTag['class'][1]\n",
    "        price = article.find('p', class_='price_color').text\n",
    "        price = float(price[1:])\n",
    "        books.append([title, star, price])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(books, columns=['Title', 'Star Rating', 'Price'])\n",
    "df.to_csv('books.csv')\n",
    "print(\"file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab6ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
